{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmwOCIt7oR8h1R8I1lR0h7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeliasporto/twitter-sentment-analysis/blob/primeiro-teste/tweets_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integração api twitter"
      ],
      "metadata": {
        "id": "waohLjs0oYRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "bw3rn4uXUCpo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To set your environment variables in your terminal run the following line:\n",
        "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
        "\n",
        "# bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
        "\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAG0VZgEAAAAALB4EJLZjQai5D%2FHPcZy5OT1ZV3Q%3DjXCvG7JnpnNAMK1gUh2NlufV4OcIzdkpB0Uq63IdMbLxllQ9Lg'\n",
        "\n",
        "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "\n",
        "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
        "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
        "query_params = {'query': '(from:twitterdev -is:retweet) OR #twitterdev','tweet.fields': 'author_id'}"
      ],
      "metadata": {
        "id": "CTeVgyDCUMFQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bearer_oauth(r):\n",
        "    \"\"\"\n",
        "    Method required by bearer token authentication.\n",
        "    \"\"\"\n",
        "\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
        "    return r\n",
        "\n",
        "def connect_to_endpoint(url, params):\n",
        "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
        "    print(response.status_code)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def main():\n",
        "    json_response = connect_to_endpoint(search_url, query_params)\n",
        "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL5kMjBzS48T",
        "outputId": "da7c07a5-ab9b-4e49-bba5-8dea55173c92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "{\n",
            "    \"data\": [\n",
            "        {\n",
            "            \"author_id\": \"1143906097\",\n",
            "            \"id\": \"1509927391241641997\",\n",
            "            \"text\": \"#twitterdeveloper #twitterdev #twitterapi\\nwhy can I only make a read-only token with OAuth1.0? on my last app I made I could make tokens with Read Write permissions\"\n",
            "        },\n",
            "        {\n",
            "            \"author_id\": \"1217873425051447296\",\n",
            "            \"id\": \"1509818098949685249\",\n",
            "            \"text\": \"RT @skolo_online: How can I use the Twitter API to get more followers? \\n#TwitterAPI #API #TwitterDev #programmingcourses #udemycourses #Cou\\u2026\"\n",
            "        },\n",
            "        {\n",
            "            \"author_id\": \"1268805305846181891\",\n",
            "            \"id\": \"1509817604621451297\",\n",
            "            \"text\": \"How can I use the Twitter API to get more followers? \\n#TwitterAPI #API #TwitterDev #programmingcourses #udemycourses #Course MUFASA IS 27 Blac Chyna Nets Moon Knight Thief\"\n",
            "        },\n",
            "        {\n",
            "            \"author_id\": \"1506293366102835211\",\n",
            "            \"id\": \"1509615137195499525\",\n",
            "            \"text\": \"&lt;a href=\\\"https://t.co/7bO0Pj2AEn\\\" class=\\\"twitter-hashtag-button\\\" data-show-count=\\\"false\\\"&gt;Tweet #TwitterDev&lt;/a&gt;&lt;script async src=\\\"https://t.co/b1qiW1VV7n\\\" charset=\\\"utf-8\\\"&gt;&lt;/script&gt;#TwitterDev\"\n",
            "        },\n",
            "        {\n",
            "            \"author_id\": \"2244994945\",\n",
            "            \"id\": \"1509259456525193221\",\n",
            "            \"text\": \"\\ud83d\\udce2 Learn how you can get historical Twitter data for research using our new Downloader tool without writing any code.\\n\\nJoin us tomorrow at 2pm PT on \\u27a1\\ufe0f https://t.co/GrtBOXh5Y1 https://t.co/uXOgMNLMry\"\n",
            "        },\n",
            "        {\n",
            "            \"author_id\": \"2244994945\",\n",
            "            \"id\": \"1508565972147326977\",\n",
            "            \"text\": \"Join @andypiper and team this Wednesday for the last installment of our developer Spaces series on custom Tweet card formats.\\n\\nhttps://t.co/WqsF8hEBSs\"\n",
            "        }\n",
            "    ],\n",
            "    \"meta\": {\n",
            "        \"newest_id\": \"1509927391241641997\",\n",
            "        \"oldest_id\": \"1508565972147326977\",\n",
            "        \"result_count\": 6\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalização"
      ],
      "metadata": {
        "id": "MPKOk01xoVbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph6UuQRIo50v",
        "outputId": "34a4cd83-59eb-4b8a-db11-4b8a33b883c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 8.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=2a543b702766d88248111e0b09bf5e8ec06ad8cbcade19db055d6c39c28d5e4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faj4i8iDpiKC",
        "outputId": "d05c4231-ab2a-44b2-883f-fa42b790f87a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 48.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import contractions"
      ],
      "metadata": {
        "id": "GxaaxWdIS5Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_retweet(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('RT\\s+', default_replace, tweet)\n",
        "  return tweet\n",
        "  "
      ],
      "metadata": {
        "id": "h1t4h4C-ogT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demojize(tweet):\n",
        "  tweet = emoji.demojize(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "ufqKbfvypLKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_url(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "EXbezgjXpRA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_hashtag(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('#+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "F2KbWn_1pTtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "KpnZ8vzopXSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_repetition(tweet):\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "uY-axa2Apa-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punct_repetition(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "N9-vGjkzpduq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _fix_contractions(tweet):\n",
        "  for k, v in contractions.contractions_dict.items():\n",
        "    tweet = tweet.replace(k, v)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "GUq1gP3OpjkN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_contractions(tweet):\n",
        "  tweet = contractions.fix(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "XxXvYxrCpsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenization"
      ],
      "metadata": {
        "id": "dIg5mretp_xq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAGSohY5PcV"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc6naeSUkNs8"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MBBQJDeQSi4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXqXrG42qMk"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcgpw29wmec4"
      },
      "source": [
        "stop_words.discard('not')\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6cZbaTB5Yr7"
      },
      "source": [
        "def custom_tokenize(tweet,\n",
        "                    keep_punct = False,\n",
        "                    keep_alnum = False,\n",
        "                    keep_stop = False):\n",
        "  \n",
        "  token_list = word_tokenize(tweet)\n",
        "\n",
        "  if not keep_punct:\n",
        "    token_list = [token for token in token_list\n",
        "                  if token not in string.punctuation]\n",
        "\n",
        "  if not keep_alnum:\n",
        "    token_list = [token for token in token_list if token.isalpha()]\n",
        "  \n",
        "  if not keep_stop:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.discard('not')\n",
        "    token_list = [token for token in token_list if not token in stop_words]\n",
        "\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXyaftGrEvkC"
      },
      "source": [
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, \n",
        "                                                keep_punct=True, \n",
        "                                                keep_alnum=True, \n",
        "                                                keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_alnum=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatization"
      ],
      "metadata": {
        "id": "zA7JzFRTqWNS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyvd4Cajqpbr"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvX7fO6Sqpbt"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00zE5WXbqpbt"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSKXzISCqpbu"
      },
      "source": [
        "stop_words.discard('not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFzi3Euzqpbv"
      },
      "source": [
        "def custom_tokenize(tweet,\n",
        "                    keep_punct = False,\n",
        "                    keep_alnum = False,\n",
        "                    keep_stop = False):\n",
        "  \n",
        "  token_list = word_tokenize(tweet)\n",
        "\n",
        "  if not keep_punct:\n",
        "    token_list = [token for token in token_list\n",
        "                  if token not in string.punctuation]\n",
        "\n",
        "  if not keep_alnum:\n",
        "    token_list = [token for token in token_list if token.isalpha()]\n",
        "  \n",
        "  if not keep_stop:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.discard('not')\n",
        "    token_list = [token for token in token_list if not token in stop_words]\n",
        "\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_OfkMGNqpbw"
      },
      "source": [
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, \n",
        "                                                keep_punct=True, \n",
        "                                                keep_alnum=True, \n",
        "                                                keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_alnum=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdQlhSQUL7T"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljxJlz0yJ8H"
      },
      "source": [
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWHJMdHH18le"
      },
      "source": [
        "word_type = {\"international\": wordnet.ADJ, \n",
        "             \"companies\": wordnet.NOUN, \n",
        "             \"had\": wordnet.VERB, \n",
        "             \"interns\": wordnet.NOUN\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-AAGLN4UWTO"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Kiw9xOZE-F"
      },
      "source": [
        "def lemmatize_tokens(tokens, word_type, lemmatizer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tudo junto"
      ],
      "metadata": {
        "id": "mDhwIMaYq-GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQofxMv37Ik"
      },
      "source": [
        "def process_tweet(tweet, verbose=False):\n",
        "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Twitter Features\n",
        "  tweet = replace_retweet(tweet) # replace retweet\n",
        "  tweet = replace_user(tweet, \"\") # replace user tag\n",
        "  tweet = replace_url(tweet) # replace url\n",
        "  tweet = replace_hashtag(tweet) # replace hashtag\n",
        "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Word Features\n",
        "  tweet = to_lowercase(tweet) # lower case\n",
        "  tweet = fix_contractions(tweet) # replace contractions\n",
        "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
        "  tweet = word_repetition(tweet) # replace word repetition\n",
        "  tweet = demojize(tweet) # replace emojis\n",
        "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Tokenization & Stemming\n",
        "  tokens = custom_tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
        "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
        "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
        "\n",
        "  return stem"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}